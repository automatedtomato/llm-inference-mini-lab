method: "bitsandbytes"
load_in_4bit: true
bnb_4bit_quant_type: "nf4"
bnb_4bit_compute_dtype: "float16"
fp16_layers:
    - "model.layers.4.mlp.down_proj"
    - "model.layers.4.self_attn.o_proj"
    - "model.layers.5.mlp.down_proj"
    - "model.layers.5.self_attn.o_proj"